# 为什么机器学习解决网络安全问题总是失败：不合理的评估指标

本文的所有内容与作者的日常工作无关，其观点也仅代表作者个人意见，与作者的雇主无关。

English version is available at <https://toooold.com/2021/12/12/why_ml_fails_security_evaluation_en.html>

网络安全和风险控制行业一向被认为是消耗商业价值的成本中心，所谓“安全一上，怨声载道。风控一拦，市场白玩”。与此同时，安全从业者需要通过保证系统和业务的整体安全以保持可持续的长期商业价值，毕竟靠黑产薅羊毛刷起来的日活和营收总有一天会以更高的代价还回去。从网络安全对长期商业价值的意义这一角度出发，我们可以讨论一下机器学习解决网络安全问题的第三大失败原因，不合理的评估指标。简单来说，我们在设定数据模型的评估指标时，有时候忘记了长期商业价值这一根本出发点。

文中关于设计评估指标的讨论在学术界并不多见，其原因可能来自于学届的研究的问题脱胎于具体问题并且独立于商业产品的细节，同时也有相对通用的评估指标，而工业界的具体问题与其商业价值关联更加紧密，更需要数据科学家将这些通用的指标具体化并关联到商业价值。

## 为什么需要合理的评估指标

合理的评估指标为数据和安全模型在达成目标的道路上提供指导方向。对指标的提升可以直接映射到行业内的商业价值，从而驱动数据模型和安全模型有的放矢的提升，同时其带来的商业价值也保证对模型的持续投入，比如提升 1% 的恶意软件检测率可以避免感染成千上万台云主机，缩短 0.01 秒的 WAF 检测时间会提高客户主机网络吞吐量的阈值以更有效的抵御攻击风险等等。

网络安全行业需要在动态且强对抗环境下解决安全问题，由攻击方或者环境带来的不确定性也会带来设定评估指标的困扰。例如对入侵检测模型的评估，如果我的业务结构没有受到有效的攻击，这是因为我的检测模型做得好，还是因为对方没有能攻破前几层防线，或者干脆就懒得攻击我，甚至是其实被攻破了只是我不知道？这些对抗和动态环境使得数据科学团队在构建模型时常陷入两难境地，一方面想检测出更多的攻击，一方面想保证更好的防御，可是更好的防御意味着更少的攻击，那么如何如何评估防御指标？同样的困扰也存在于各个风险控制团队、漏洞巡视和检测团队等等。“善战者无赫赫之功”`*`，我们如何更好的构建和评估检测和防御体系呢？

在实际的工作中，作者发现设定合理的评估指标需要面临诸多挑战：那些不能正确反映长期商业价值的评估指标也往往错误的指引了数据和安全模型的研究方向，这些指标也常常挑起商业增长与安全防护的矛盾，更有甚者，部分从业人员迫于不合理的指标带来的压力而使用非常手段来利用指标的漏洞，使得模型和产品功能偏离其设定方向。

总的来说，合理的评估指标是连接优秀的建模工作和其商业价值的重要桥梁，它有效指引了模型工作的方向，而不合理的评估指标会让优秀的模型在错误的方向上努力，其不令人满意的结果也让建模工作承担不必要的责难。

## 错误之一：失去目标的指标

目标和指标的关系是数据科学基础知识之一，但这种“失去目标的指标”错误几乎占了不合理指标的绝大多数情况！

各位小伙伴在上课时有没有想过这个问题：既然判别模型为了追求准确，那机器学习模型为什么不用准确率代替目标损失函数进行优化呢？`*` 抛开其背后的统计和数学原因（包括假设、后验和先验等以及他们的实际意义），直观的理解可以是，损失函数定义目标的优化方向，而准确率等指标评估其优化完毕时结果的好坏。准确率只能被人用来评估机器预测（指标）是不能被机器拿来判断对错（目标），否则机器会失去损失函数降低带来的优化方向而陷入它误以为的最优解。这也对应了人工智能课程提到的决策的基本原则：智能体需要做明智的决策而不仅是结果正确的决策。

但是聪明的人类在决策过程中却因为利益等原因混淆了目标和指标。我们见过很多因为考试作弊没有被抓而洋洋得意的学生最终的失利，也见过为了单日活跃用户数发出大量红包，但没有足以留存用户的产品功能而最终流失用户的各大APP。一时的考试成绩和几天的日活数字只是指标，指标只能在“牢固知识”和“构建好产品”这些目标下才有意义。

网络安全团队和网络安全产品的目标是为了保障自身和客户的资产免受网络攻击的侵害，在这一目标下，不同的领域有不同的子目标，以及对应的指标以衡量目标的达成情况。业界有很多指标不反映目标的情况，例如某 WAF 产品以自己每天为客户防御多少亿次攻击为指标，而不是以产品的易用易部署、低成本高吞吐、低延迟等更能反映其商业目标的指标。这样的“防御多少亿次攻击”的“想当然”的指标看似容易量化，但其荒谬程度就好比某消防站以扑灭多少次火灾为绩效考核标准一样，失去目标的指标对商业价值没有意义。

以这样的不合理指标评估的工作甚至会带来负面影响：它会在错误优化方向上浪费人力和计算资源，也变相鼓励短期效益忽略长期目标，甚至有时候它甚至纵容玩弄评估系统和弄虚作假。如果用威胁的覆盖率作为指标，那么模型可以认为所有活动均为恶意行为，并将大量事件输出给安全运营团队处理；如果用检测准确率作为指标，那么模型最好什么都不汇报，只要不预测就不会犯错；如果用告警量作为指标，那么模型会不加甄别的发送海量告警，只要足够多就可以拖垮客户运营团队让他们没时间投诉。可以对这些看似无理取闹的行为在实际工作中以不同形式真实存在。

## 错误之二：机械套用常规指标

基于统计的机器学习判别模型是为了学习目标分布的期望而设计的，它暗示着算法总是被激励去预测多数群体的行为`*`，因为多数群体主导了目标分布的统计期望。如果机械套用常规的准确率召回率指标，而非理解算法更倾向于寻找多数群体行为并按照特定问题设计符合该问题的指标，不仅不能解决问题，反而会让人们对算法的有效性产生疑问。

网络安全中攻击事件的发生频率分布极度不平衡，攻击事件往往只有千万分之一的概率出现，同时每种攻击事件发现的难度千差万别，如果想当然的要求判别模型达到对攻击事件有90%的准确率，那么模型最好就什么都不检测，因为负样本比正样本高出若干数量级，单个样本的误判足以将准确率降低到接近于0，这类问题已经不能通过常规的非平衡样本方法解决。

网络安全的各种情况里，多数情况缺少基准事实（ground truth），例如0day漏洞的发现，APT攻击等，在这种情况下对数据模型要求所谓的召回率，甚至所谓“未知威胁的召回率”，这样的指标可以说“连错误都算不上”（“not even wrong”）。"世界上只有两种公司，一种被黑客入侵过，另一种将被入侵。“`*` 我们同样也不能等待自己被入侵以计算召回率。入侵攻击事件的对商业的效果有很大延迟，比如若干年后的数据泄露，或者暗网上正在出售已泄露的数据而安全团队依然不知道。如果为了追求基准事实而仅仅依赖某些攻击评测手段，例如邀请蓝军攻击等，其受限的攻击场景也会片面评估模型的效果。如果数据科学团队因为任何原因应允了类似的指标，团队会为此付出大量的人力和资源，最终以不能解决问题而失败收场。

除了常规的准确率召回率等指标，数据模型还应该有面对未知情况的茁壮性、可解释性、可运营条件等，否则该模型的有效性只停留在已知的固定数据集而不能成为可靠的生产环境流程。

## 错误之三：独立检出的诅咒

检测类的模型是机器学习模型在网络安全行业的热门话题，例如恶意二进制文件/脚本检测、钓鱼页面检测等，其超越已有规则模型或者第三方情报的独立检出常常被用来当作评估指标。这个看似合理的指标在实际工作中带来了不少的问题，不限于以下这些：

* 检出样本的商业价值更多在其可以影响的业务资产而非样本个数，评估过程也忽略了检出时间的先后次序带来的影响。
* 缺失准确率等质量评估的规则模型的结果作为分母不足以合理的计算独立检出率
* 使用完全不同方法的规则与机器学习模型的结果常有大量重合，仅评估机器学习模型而忽视规则模型的独立检出指标，这也常引发评估公正性的讨论。

本文作者甚至观察到，某些安全团队一方面排斥数据模型的检测结果，一方面从数据模型的结果提取规则加入自己的检测库，通过提高分母的办法让数据科学团队的独立检出率保持在较低水平。安全团队口中的“机器学习没有用”和数据科学团队提出的“安全团队又当运动员又当裁判员”等观点均来源于此，这些无意义的内部竞争消耗了多个团队的精力和信任，最终造成了公司层面的人员流失和经济损失。独立检出这一指标带来了割裂团队阻止合作的诅咒。

## 荣誉提名：正确的指标，错误的问题

我们在实际工作中也观察到，有些网络安全问题问题本身不适合机器学习和人工智能，比如利用第三方情报检测未知 APT 攻击等目标；想要构建基于日志的威胁发现，然而忽略了所需要的数据采集和数据仓库工作；某些问题本身需要巨大投入，而现有资源不足以支撑，最常见的是各个公司热衷于自研反病毒引擎；或者是该问题本身并不存在，比如说机器学习生成安全运营的告警白名单，而白名单本身就是个伪命题。这些问题都可以设立明确的指标，但是其目标本身是个错误的问题，最终导致数据科学团队无功而返。

## 一些设计评估指标的建议

所有的指标必须以目标为前提。目标定义了解决问题的有限责任，只有在有限责任下才可以提出合理的指标。我们必须总是保证目标优先，而指标只是在保证目标时候的关键结果，需要理解商业需求制定目标而非拍脑袋拍出一个看似有道理的指标，数据科学家也需要清晰鉴别此类拍脑袋的评估标准并及时提出反馈。

在规划问题和设定目标时，应该评估该目标是否过大或者过小，该场景是否适合使用该解决方案，以及该解决方案的目标是否在合理的资源预算内。建议在规划对比业界一般解决方案和自身特定问题，按照当前情况合理安排资源。

独立检出一般是个很坏的指标，把数据模型和规则模型或者外部采购放到了对立面，同时忽略了检出样本对资产的影响以及检测时间先后等因素。对于检测类的模型，我们尽量避免将独立检出作为指标，而使用交集并集看检出结果的总体覆盖率和对资产的影响；如需对比模型应该看检测时间先后而非鼓励规则模型获取独检结果后更新规则以取代数据模型；同时考虑到作为基础模型的规则模型解决的是该问题较为容易部分，机器学习模型的独立检出应该以大于零为指标，并考虑下一轮迭代更新的代价。

如果没有基准事实或攻击方测试怎么办？在缺少基准事实的情况下，尽可能多的异常检测以及尽可能多的解释这些异常发生的原因，能够解释异常结果的召回率可能是更好的评估指标。在缺少攻击方测试的情况下，可以利用防守方对资产所需的防守面的覆盖程度评估攻击检测的指标。在网络安全这一动态对抗环境下，我们也必须主动且及时调整评估策略。

## 总结

合理的评估指标可有效的促进数据和安全模型在其业务领域体现商业价值，我们需要设定符合目标的合理评估指标。数据科学团队也需要深刻理解算法总是被激励去预测多数群体的行为，并合理设计评价指标以发挥算法模型的优势。

合理的指标也可以避免对模型的无谓优化甚至错误优化。无论该模型的优化目标是否正确合理，聪明的数据科学家可以将建模工作做的很出色，而脱离了合理的指标，优化的越好带来的错误就越多，其最终带来的商业损失和工作的挫败感需要更多的代价来平复。

## 参考文献

* 曹操批注孙子兵法，“善战者无赫赫之功”
* Quora "Why do we use loss functions in machine learning instead of simply optimizing for accuracy?" <https://www.quora.com/Why-do-we-use-loss-functions-in-machine-learning-instead-of-simply-optimizing-for-accuracy>
* The Myth of the
Impartial Machine <https://parametric.press/issue-01/the-myth-of-the-impartial-machine/>
* Not even wrong <https://en.wikipedia.org/wiki/Not_even_wrong>
* "There are only two types of companies: Those that have been hacked and those that will be hacked." – Robert Mueller, former Director of the FBI